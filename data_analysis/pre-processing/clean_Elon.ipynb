{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Elon"
      ],
      "metadata": {
        "id": "efoSe1fntD1Y"
      },
      "id": "efoSe1fntD1Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7804c9e",
      "metadata": {
        "id": "c7804c9e",
        "outputId": "e6a762a3-7db6-4625-bcf0-6c84b7f92858"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\flori\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# utilities\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "# nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# defining a stop word list to remove unnecessary word that do not hold much informational value for sentiment analysis\n",
        "#stopwordlist = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "STOPWORDS = set(stopwordlist)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*kursiver Text*# Clean Elon"
      ],
      "metadata": {
        "id": "Ic2ZLPxgsyZM"
      },
      "id": "Ic2ZLPxgsyZM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4f618b",
      "metadata": {
        "id": "7e4f618b"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweets(tweet):\n",
        "    # converting tweet text to lower case for better analysis\n",
        "    tweet = tweet.lower()\n",
        "    # removing every URL starting with www. or http(s)://\n",
        "    tweet = re.sub('((www.[^s]+)|(https?://[^s]+))',' ',tweet)\n",
        "    # removing every Twitter handle with @ at the beginning\n",
        "    tweet = re.sub(r'@[^ ]+', '', tweet)\n",
        "    # clearing out numerical values\n",
        "    tweet = re.sub('[0-9]+', '', tweet)\n",
        "    # removing punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "   \n",
        "    # tokenizing the tweet\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    # defining a stop word list to remove unnecessary word that do not hold much informational value for sentiment analysis\n",
        "    stopword_filter = [w for w in tweet_tokens if not w in STOPWORDS]\n",
        "    \n",
        "    # initialising NLTK stemmer\n",
        "    ps = PorterStemmer()\n",
        "    # stemming the filtered tokens\n",
        "    stemmed_words = [ps.stem(w) for w in stopword_filter]\n",
        "    \n",
        "    # initialising lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # lemmatisation of stemmed tokens\n",
        "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
        "    \n",
        "    return \" \".join(stopword_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d5455f",
      "metadata": {
        "id": "e5d5455f"
      },
      "outputs": [],
      "source": [
        "# input file path\n",
        "input_path = input(\"Enter input path of csv file\")\n",
        "input_path = re.sub(r\"\\\\\", \"/\", input_path)\n",
        "input_path = re.sub('\"', \"\", input_path)\n",
        "\n",
        "# output file path\n",
        "output_path = input_path[:-24]\n",
        "output_path = re.sub(r\"\\\\\", \"/\", output_path)+(\"cleaned\")\n",
        "output_name = input_path[-24:-4]+(\"_clean\")\n",
        "output_file = output_path+(\"/\")+output_name+(\".csv\")\n",
        "\n",
        "# loading the data set\n",
        "df = pd.read_csv(input_path).drop(['Unnamed: 0'], axis=1)\n",
        "# removing non-ASCII characters\n",
        "df['original_tweet'] = df['original_tweet'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
        "\n",
        "df.original_tweet = df['original_tweet'].apply(preprocess_tweets)\n",
        "df = df[df['original_tweet'].str.contains(\"doge\")]\n",
        "\n",
        "df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5c8d11",
      "metadata": {
        "id": "0f5c8d11"
      },
      "outputs": [],
      "source": [
        "os.chdir(output_path)\n",
        "\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "#combine all files in the list\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv_new.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b593bcc8",
      "metadata": {
        "id": "b593bcc8"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"C:/Users/flori/OneDrive/Master Studium/Social Media Project/Elon Musk/2019\")\n",
        "\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "\n",
        "#combine all files in the list\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv(\"combined_raw__tweets.csv\", index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74578cac",
      "metadata": {
        "id": "74578cac"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New clean\n"
      ],
      "metadata": {
        "id": "HnRjdW6is31G"
      },
      "id": "HnRjdW6is31G"
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "import re\n",
        "\n"
      ],
      "metadata": {
        "id": "6XO_RyITtOqv"
      },
      "id": "6XO_RyITtOqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweets(tweet):\n",
        "    # Replace multiple spaces with single spaces.\n",
        "    tweet = re.sub(' +', ' ', tweet)\n",
        "    # Convert two or more dot's into one.\n",
        "    tweet = re.sub(r'\\.+', \".\", tweet)\n",
        "    # replace \"&amp\" into &\n",
        "    tweet = tweet.replace(\"&amp;\", \"&\")\n",
        "\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "xCYqXXZps9Oo"
      },
      "id": "xCYqXXZps9Oo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data set\n",
        "df = pd.read_excel(\"/content/doge_coin_cleaned_dataset_2020.xlsx\")\n",
        "\n",
        "df['cleaned_tweet'] = df['cleaned_tweet'].apply(clean_tweets)\n",
        "df.head(151)\n",
        "\n",
        "#df.to_csv(\"/content/doge_coin_cleaned_dataset_2020_cleaner.xlsx\", index = False)"
      ],
      "metadata": {
        "id": "u3TdJZ9St5uk"
      },
      "id": "u3TdJZ9St5uk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tE2bsen_H4gT"
      },
      "id": "tE2bsen_H4gT"
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "import re\n",
        "\n",
        "def clean_tweets(tweet):\n",
        "    # Replace multiple spaces with single spaces.\n",
        "    tweet = re.sub(' +', ' ', tweet)\n",
        "    # Convert two or more dot's into one.\n",
        "    tweet = re.sub(r'\\.+', \".\", tweet)\n",
        "    # replace \"&amp\" into &\n",
        "    tweet = tweet.replace(\"&amp;\", \"&\")\n",
        "\n",
        "    return tweet\n",
        "\n",
        "# loading the data set\n",
        "df = pd.read_excel(\"/content/doge_coin_cleaned_dataset_2020.xlsx\")\n",
        "\n",
        "df['cleaned_tweet'] = df['cleaned_tweet'].apply(clean_tweets)\n",
        "df.head(100)"
      ],
      "metadata": {
        "id": "1Zm63pcDwJ3q",
        "outputId": "85bec389-c48e-4791-b0ba-ff61b4c5191f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3778
        }
      },
      "id": "1Zm63pcDwJ3q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        tweet_id       user_id user_name  \\\n",
              "0   1.254114e+18  4.785164e+08    Nobody   \n",
              "1   1.254113e+18  1.234792e+18    Nobody   \n",
              "2   1.254112e+18  2.232163e+09    Nobody   \n",
              "3   1.254111e+18  1.220623e+18    Nobody   \n",
              "4   1.254110e+18  5.364488e+08    Nobody   \n",
              "..           ...           ...       ...   \n",
              "95  1.284332e+18  1.204477e+18    Nobody   \n",
              "96  1.284332e+18  1.233160e+18    Nobody   \n",
              "97  1.284331e+18  9.136012e+17    Nobody   \n",
              "98  1.284331e+18  3.041968e+09    Nobody   \n",
              "99  1.284330e+18  7.605815e+17    Nobody   \n",
              "\n",
              "                                       original_tweet    year  month   day  \\\n",
              "0   I'm in the running to #win 100 #Dogecoin thank...  2020.0    4.0  25.0   \n",
              "1   🔶 Mining-up Site 🔶\\n🔸Free 50GH/s🔸\\n🔸#Mining $B...  2020.0    4.0  25.0   \n",
              "2   1 DOGE Price: 0.00000028 BTC #doge #dogecoin 2...  2020.0    4.0  25.0   \n",
              "3   Free mining new site..\\n\\nFree Reg bonus : 100...  2020.0    4.0  25.0   \n",
              "4   I'm in the running to #win 100 #Dogecoin thank...  2020.0    4.0  25.0   \n",
              "..                                                ...     ...    ...   ...   \n",
              "95  @KEEMSTAR Idk about you guys but.. DID YOU KNO...  2020.0    7.0  18.0   \n",
              "96  @elonmusk lets pump it up.... my kids gotta ea...  2020.0    7.0  18.0   \n",
              "97  All hail the #dogecoin only the best friend of...  2020.0    7.0  18.0   \n",
              "98  @elonmusk Lemme know when we can buy a @Tesla ...  2020.0    7.0  18.0   \n",
              "99  I told people @elonmusk is a professional trol...  2020.0    7.0  18.0   \n",
              "\n",
              "        time               source  \\\n",
              "0   18:25:09  Twitter for Android   \n",
              "1   18:20:23  Twitter for Android   \n",
              "2   18:15:13           CoinTweety   \n",
              "3   18:11:03  Twitter for Android   \n",
              "4   18:07:47      Twitter Web App   \n",
              "..       ...                  ...   \n",
              "95  03:40:36  Twitter for Android   \n",
              "96  03:39:35  Twitter for Android   \n",
              "97  03:37:40   Twitter for iPhone   \n",
              "98  03:34:24  Twitter for Android   \n",
              "99  03:32:13  Twitter for Android   \n",
              "\n",
              "                                        cleaned_tweet  \n",
              "0   I'm in the running to #win 100 #Dogecoin thank...  \n",
              "1   🔶 Mining-up Site 🔶\\n🔸Free 50GH/s🔸\\n🔸#Mining $B...  \n",
              "2   1 DOGE Price: 0.00000028 BTC #doge #dogecoin 2...  \n",
              "3   Free mining new site.\\n\\nFree Reg bonus : 1000...  \n",
              "4   I'm in the running to #win 100 #Dogecoin thank...  \n",
              "..                                                ...  \n",
              "95  @KEEMSTAR Idk about you guys but. DID YOU KNOW...  \n",
              "96  @elonmusk lets pump it up. my kids gotta eat. ...  \n",
              "97  All hail the #dogecoin only the best friend of...  \n",
              "98  @elonmusk Lemme know when we can buy a @Tesla ...  \n",
              "99  I told people @elonmusk is a professional trol...  \n",
              "\n",
              "[100 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65958ba9-adc1-4668-8f77-089b515bf6ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_name</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>source</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.254114e+18</td>\n",
              "      <td>4.785164e+08</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>I'm in the running to #win 100 #Dogecoin thank...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>18:25:09</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>I'm in the running to #win 100 #Dogecoin thank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.254113e+18</td>\n",
              "      <td>1.234792e+18</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>🔶 Mining-up Site 🔶\\n🔸Free 50GH/s🔸\\n🔸#Mining $B...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>18:20:23</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>🔶 Mining-up Site 🔶\\n🔸Free 50GH/s🔸\\n🔸#Mining $B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.254112e+18</td>\n",
              "      <td>2.232163e+09</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>1 DOGE Price: 0.00000028 BTC #doge #dogecoin 2...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>18:15:13</td>\n",
              "      <td>CoinTweety</td>\n",
              "      <td>1 DOGE Price: 0.00000028 BTC #doge #dogecoin 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.254111e+18</td>\n",
              "      <td>1.220623e+18</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>Free mining new site..\\n\\nFree Reg bonus : 100...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>18:11:03</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>Free mining new site.\\n\\nFree Reg bonus : 1000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.254110e+18</td>\n",
              "      <td>5.364488e+08</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>I'm in the running to #win 100 #Dogecoin thank...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>18:07:47</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>I'm in the running to #win 100 #Dogecoin thank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1.284332e+18</td>\n",
              "      <td>1.204477e+18</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>@KEEMSTAR Idk about you guys but.. DID YOU KNO...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>03:40:36</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>@KEEMSTAR Idk about you guys but. DID YOU KNOW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1.284332e+18</td>\n",
              "      <td>1.233160e+18</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>@elonmusk lets pump it up.... my kids gotta ea...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>03:39:35</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>@elonmusk lets pump it up. my kids gotta eat. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1.284331e+18</td>\n",
              "      <td>9.136012e+17</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>All hail the #dogecoin only the best friend of...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>03:37:40</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>All hail the #dogecoin only the best friend of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1.284331e+18</td>\n",
              "      <td>3.041968e+09</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>@elonmusk Lemme know when we can buy a @Tesla ...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>03:34:24</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>@elonmusk Lemme know when we can buy a @Tesla ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1.284330e+18</td>\n",
              "      <td>7.605815e+17</td>\n",
              "      <td>Nobody</td>\n",
              "      <td>I told people @elonmusk is a professional trol...</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>03:32:13</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>I told people @elonmusk is a professional trol...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65958ba9-adc1-4668-8f77-089b515bf6ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65958ba9-adc1-4668-8f77-089b515bf6ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65958ba9-adc1-4668-8f77-089b515bf6ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UYjVdu9oHCqt"
      },
      "id": "UYjVdu9oHCqt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3f8b3182d4cfda5fcb12e28566bbdd2b1f1eec51d475e45cf0f17065ee41394"
      }
    },
    "colab": {
      "name": "clean Elon.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}