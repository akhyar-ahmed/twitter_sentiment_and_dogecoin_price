{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_bert",
      "provenance": [],
      "collapsed_sections": [
        "06kbg23epLEQ",
        "iPjB-V8oU0gm",
        "skYywirRUFjD",
        "rKcaicIFpE46",
        "lsFl0dNGmIwX",
        "Jng0RdrtecPe",
        "CikgTal4gfP9",
        "3d4aJTJogord",
        "AKpsJMP0l5aY",
        "J06DvKXHObSY"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n"
      ],
      "metadata": {
        "id": "GtciRntxOM6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsV4LG4_OQnQ",
        "outputId": "b1d5a9f0-f27e-4f88-c0df-84fa028ee90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "H5_Jnnl8OWTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZF8Lyz0Nt9-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import logging\n",
        "import threading\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import traceback\n",
        "import math\n",
        "import functools as ft\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import BertTokenizer, DistilBertTokenizer, EarlyStoppingCallback, TrainingArguments, Trainer, BertForSequenceClassification, DistilBertForSequenceClassification, EvalPrediction, BertConfig\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import preprocessing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download packages"
      ],
      "metadata": {
        "id": "06kbg23epLEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmCtFfDUUlLK",
        "outputId": "488a4ca4-8138-4170-ae8a-8d2a9f37e9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgUEzW-QUoRH",
        "outputId": "4c9f2c1c-af27-4ad4-9176-f7ddb45bebe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoLfur2LUrWy",
        "outputId": "1b4c8524-52c4-4287-a798-bcba99ec55eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ5f0T62UtiQ",
        "outputId": "5bca02d4-436a-4794-9a92-c8ce85c2aee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "iPjB-V8oU0gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "infile = \"/content/elonmusk_doge_tweets.csv\"\n",
        "# infile = \"/content/drive/MyDrive/SMBA_2022/elonmusk_doge_tweets.gsheet\"\n",
        "\n",
        "# reading the dataset\n",
        "df = pd.read_csv(infile, encoding = \"utf-8\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiXfnM6UU4W-",
        "outputId": "14968cda-c445-494e-b83c-4d8433b7d6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tweet_id                                     original_tweet  \\\n",
              "0  1.250000e+18       @28delayslater @justpaulinelol Dogecoin Mode   \n",
              "1  1.280000e+18            @izzynobre Excuse me, I only sell Doge!   \n",
              "2  1.340000e+18                                     One word: Doge   \n",
              "3  1.420000e+18  @AshleyIllusion1 @lexfridman Lil X is hodling ...   \n",
              "4  1.410000e+18  @CGDaveMac Maybe if it sees a Shiba Inu, the c...   \n",
              "\n",
              "                                      cleaned_tweeet  year  month  day  \\\n",
              "0                                      dogecoin mode  2020      4   25   \n",
              "1                       excuse me, i only sell doge!  2020      7   18   \n",
              "2                                     one word: doge  2020     12   20   \n",
              "3  lil x is hodling his doge like a champ. litera...  2021      7   17   \n",
              "4  maybe if it sees a shiba inu, the car renders ...  2021      7   13   \n",
              "\n",
              "       time  labels  \n",
              "0  13:29:52       1  \n",
              "1  00:53:43      -1  \n",
              "2  09:30:04       1  \n",
              "3  16:53:53       0  \n",
              "4  02:33:26       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49fe1f5d-2b76-417e-a2af-20effa0e2c34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>cleaned_tweeet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.250000e+18</td>\n",
              "      <td>@28delayslater @justpaulinelol Dogecoin Mode</td>\n",
              "      <td>dogecoin mode</td>\n",
              "      <td>2020</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13:29:52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.280000e+18</td>\n",
              "      <td>@izzynobre Excuse me, I only sell Doge!</td>\n",
              "      <td>excuse me, i only sell doge!</td>\n",
              "      <td>2020</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>00:53:43</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.340000e+18</td>\n",
              "      <td>One word: Doge</td>\n",
              "      <td>one word: doge</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>09:30:04</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.420000e+18</td>\n",
              "      <td>@AshleyIllusion1 @lexfridman Lil X is hodling ...</td>\n",
              "      <td>lil x is hodling his doge like a champ. litera...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>16:53:53</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.410000e+18</td>\n",
              "      <td>@CGDaveMac Maybe if it sees a Shiba Inu, the c...</td>\n",
              "      <td>maybe if it sees a shiba inu, the car renders ...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>02:33:26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49fe1f5d-2b76-417e-a2af-20effa0e2c34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49fe1f5d-2b76-417e-a2af-20effa0e2c34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49fe1f5d-2b76-417e-a2af-20effa0e2c34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the dataset"
      ],
      "metadata": {
        "id": "skYywirRUFjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing steps"
      ],
      "metadata": {
        "id": "rKcaicIFpE46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all numbers\n",
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "fBTah8krbPVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "elIvLb0cblbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize a string\n",
        "def tokenize(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return word_tokens"
      ],
      "metadata": {
        "id": "5cpe2dD2kJEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# remove stopwords function\n",
        "def remove_stopwords(word_tokens):\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "bVwFv0DkcMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize string\n",
        "def lemmatize_word(word_tokens):\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "tx8Bm-cmcRJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in df.cleaned_tweeet]\n",
        "\n",
        "print(max(seq_len))\n",
        "pd.Series(seq_len).hist(bins = 30);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ-QYG-mQzMF",
        "outputId": "32a36c41-9883-4bec-ecd5-76d5a5927782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform all the steps of pre-processing\n",
        "def do_preprocoessing(df):\n",
        "  preprocessed_tweet_ls = []\n",
        "  encoded_tweet_ls = []\n",
        "  for ix, row in df.iterrows():\n",
        "    # remove numbers\n",
        "    row.cleaned_tweeet = remove_numbers(row.cleaned_tweeet)\n",
        "\n",
        "    # remove punctuation\n",
        "    row.cleaned_tweeet = remove_punctuation(row.cleaned_tweeet)\n",
        "    \n",
        "    # tokenize a string\n",
        "    word_tokens = tokenize(row.cleaned_tweeet)\n",
        "    \n",
        "    # remove stopwords\n",
        "    word_tokens = remove_stopwords(word_tokens)\n",
        "    \n",
        "    # lemmatize a list of word\n",
        "    word_tokens = lemmatize_word(word_tokens)\n",
        "\n",
        "    # create a full string after pre-processing\n",
        "    preprocessed_tweet = \" \".join(word_tokens)\n",
        "    preprocessed_tweet_ls.append(preprocessed_tweet)\n",
        "\n",
        "  df[\"preprocessed_tweet\"] = preprocessed_tweet_ls\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "K6Q6QQYPc8lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  n_df = do_preprocoessing(df)\n",
        "except:\n",
        "  print(traceback.print_exc())"
      ],
      "metadata": {
        "id": "EVo3UbQvVAMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7j6Wj3FVPAc",
        "outputId": "e9202036-ae3f-4c91-b65e-a56c37ba5977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tweet_id                                     original_tweet  \\\n",
              "0  1.250000e+18       @28delayslater @justpaulinelol Dogecoin Mode   \n",
              "1  1.280000e+18            @izzynobre Excuse me, I only sell Doge!   \n",
              "2  1.340000e+18                                     One word: Doge   \n",
              "3  1.420000e+18  @AshleyIllusion1 @lexfridman Lil X is hodling ...   \n",
              "4  1.410000e+18  @CGDaveMac Maybe if it sees a Shiba Inu, the c...   \n",
              "\n",
              "                                      cleaned_tweeet  year  month  day  \\\n",
              "0                                      dogecoin mode  2020      4   25   \n",
              "1                       excuse me, i only sell doge!  2020      7   18   \n",
              "2                                     one word: doge  2020     12   20   \n",
              "3  lil x is hodling his doge like a champ. litera...  2021      7   17   \n",
              "4  maybe if it sees a shiba inu, the car renders ...  2021      7   13   \n",
              "\n",
              "       time  labels                                 preprocessed_tweet  \n",
              "0  13:29:52       1                                      dogecoin mode  \n",
              "1  00:53:43      -1                                   excuse sell doge  \n",
              "2  09:30:04       1                                      one word doge  \n",
              "3  16:53:53       0  lil x hodling doge like champ literally never ...  \n",
              "4  02:33:26       0          maybe see shiba inu car render dogecoin …  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5da82b3c-ad2e-4869-97a7-bf898c91003f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>cleaned_tweeet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>labels</th>\n",
              "      <th>preprocessed_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.250000e+18</td>\n",
              "      <td>@28delayslater @justpaulinelol Dogecoin Mode</td>\n",
              "      <td>dogecoin mode</td>\n",
              "      <td>2020</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13:29:52</td>\n",
              "      <td>1</td>\n",
              "      <td>dogecoin mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.280000e+18</td>\n",
              "      <td>@izzynobre Excuse me, I only sell Doge!</td>\n",
              "      <td>excuse me, i only sell doge!</td>\n",
              "      <td>2020</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>00:53:43</td>\n",
              "      <td>-1</td>\n",
              "      <td>excuse sell doge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.340000e+18</td>\n",
              "      <td>One word: Doge</td>\n",
              "      <td>one word: doge</td>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>09:30:04</td>\n",
              "      <td>1</td>\n",
              "      <td>one word doge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.420000e+18</td>\n",
              "      <td>@AshleyIllusion1 @lexfridman Lil X is hodling ...</td>\n",
              "      <td>lil x is hodling his doge like a champ. litera...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>16:53:53</td>\n",
              "      <td>0</td>\n",
              "      <td>lil x hodling doge like champ literally never ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.410000e+18</td>\n",
              "      <td>@CGDaveMac Maybe if it sees a Shiba Inu, the c...</td>\n",
              "      <td>maybe if it sees a shiba inu, the car renders ...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>02:33:26</td>\n",
              "      <td>0</td>\n",
              "      <td>maybe see shiba inu car render dogecoin …</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5da82b3c-ad2e-4869-97a7-bf898c91003f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5da82b3c-ad2e-4869-97a7-bf898c91003f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5da82b3c-ad2e-4869-97a7-bf898c91003f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### positive, negative and neutral df"
      ],
      "metadata": {
        "id": "lsFl0dNGmIwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(n_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FX8o2bLZCfg",
        "outputId": "baa4d5db-6869-4b6a-90a3-edaa4f8bf795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df = n_df[n_df.labels==1]\n",
        "pos_df = pos_df.sample(frac=1)\n",
        "len(pos_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9okceCvYvZ8",
        "outputId": "68821dd7-f865-4c52-88f6-191e1b430afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_df = n_df[n_df.labels==-1]\n",
        "neg_df = neg_df.sample(frac=1)\n",
        "len(neg_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWmBLuTDZGnr",
        "outputId": "c0a5a71f-cf01-4ad1-a9aa-0a1f829c0407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neu_df = n_df[n_df.labels==0]\n",
        "neu_df = neu_df.sample(frac=1)\n",
        "len(neu_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9ZYQ8gZHgE",
        "outputId": "76046d62-7630-45b9-f928-837361313f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_percentage = 60\n",
        "test_percentage = 20\n",
        "valid_percentage = 20"
      ],
      "metadata": {
        "id": "qCkgv0VodYIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate class ratios for pos"
      ],
      "metadata": {
        "id": "Jng0RdrtecPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_train_n = len(pos_df)*(train_percentage/100)\n",
        "if (pos_train_n - int(pos_train_n)) >= 0.5:\n",
        "  pos_train_n = int(pos_train_n)+1\n",
        "else:\n",
        "  pos_train_n = int(pos_train_n)\n",
        "pos_train_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQMjUXqZ83D",
        "outputId": "a0dda33a-6491-47e4-f126-49e817875324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_test_n = len(pos_df)*(test_percentage/100)\n",
        "if (pos_test_n - int(pos_test_n)) >= 0.5:\n",
        "  pos_test_n = int(pos_test_n)+1\n",
        "else:\n",
        "  pos_test_n = int(pos_test_n)\n",
        "pos_test_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRawWwA0fWpa",
        "outputId": "ba5ba2f8-d4e5-4ce0-87e8-d91512a4b120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_valid_n = len(pos_df)*(valid_percentage/100)\n",
        "if (pos_valid_n - int(pos_valid_n)) >= 0.5:\n",
        "  pos_valid_n = int(pos_valid_n)+1\n",
        "else:\n",
        "  pos_valid_n = int(pos_valid_n)\n",
        "pos_valid_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heD1zIapbble",
        "outputId": "ccd843e1-73a6-4c0d-d181-d6d5a43e0a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate class ratios for neg"
      ],
      "metadata": {
        "id": "CikgTal4gfP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg_train_n = len(neg_df)*(train_percentage/100)\n",
        "if (neg_train_n - int(neg_train_n)) >= 0.5:\n",
        "  neg_train_n = int(neg_train_n)+1\n",
        "else:\n",
        "  neg_train_n = int(neg_train_n)\n",
        "neg_train_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un5kPwEPelOK",
        "outputId": "b7287709-eff2-4d02-d5cb-1eec27e00b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_test_n = len(neg_df)*(test_percentage/100)\n",
        "if (neg_test_n - int(neg_test_n)) >= 0.5:\n",
        "  neg_test_n = int(neg_test_n)+1\n",
        "else:\n",
        "  neg_test_n = int(neg_test_n)\n",
        "neg_test_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAIf4n5ifWFY",
        "outputId": "c976ec9f-52a5-4ace-d96e-57df0a0b104d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_valid_n = len(neg_df)*(valid_percentage/100)\n",
        "if (neg_valid_n - int(neg_valid_n)) >= 0.5:\n",
        "  neg_valid_n = int(neg_valid_n)+1\n",
        "else:\n",
        "  neg_valid_n = int(neg_valid_n)\n",
        "neg_valid_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-87YYdg2F2",
        "outputId": "ebc5a822-8e34-4e48-fe94-0125e0951e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate class ratios for neu"
      ],
      "metadata": {
        "id": "3d4aJTJogord"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neu_train_n = len(neu_df)*(train_percentage/100)\n",
        "if (neu_train_n - int(neu_train_n)) >= 0.5:\n",
        "  neu_train_n = int(neu_train_n)+1\n",
        "else:\n",
        "  neu_train_n = int(neu_train_n)\n",
        "neu_train_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si1XkU31el4G",
        "outputId": "963aacbf-ee32-45ee-b6eb-10fa721a6c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neu_test_n = len(neu_df)*(test_percentage/100)\n",
        "if (neu_test_n - int(neu_test_n)) >= 0.5:\n",
        "  neu_test_n = int(neu_test_n)+1\n",
        "else:\n",
        "  neu_test_n = int(neu_test_n)\n",
        "neu_test_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIxBf5rqhBTM",
        "outputId": "69be5a51-7c60-479e-f9e9-b4fe8f5f9896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neu_valid_n = len(neu_df)*(valid_percentage/100)\n",
        "if (neu_valid_n - int(neu_valid_n)) >= 0.5:\n",
        "  neu_valid_n = int(neu_valid_n)+1\n",
        "else:\n",
        "  neu_valid_n = int(neu_valid_n)\n",
        "neu_valid_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLbEOM05buKy",
        "outputId": "85cc268a-107b-4ba7-b701-dd5d3b2d7996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train, test and valid df"
      ],
      "metadata": {
        "id": "AKpsJMP0l5aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_df = pos_df.iloc[0:pos_train_n, 0:]\n",
        "train_neg_df = neg_df.iloc[0:neg_train_n, 0:]\n",
        "train_neu_df = neu_df.iloc[0:neu_train_n, 0:]\n",
        "train_df = train_pos_df.append([train_neg_df, train_neu_df]).sample(frac=1)\n",
        "train_df.labels = train_df.labels.replace({-1:0, 0:1, 1:2})\n",
        "\n",
        "print(len(train_df))\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQDIJL3bdogh",
        "outputId": "28e3984a-c2ca-4928-9a99-8a93c95f7990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        tweet_id                                     original_tweet  \\\n",
              "26  1.390000e+18  Working with Doge devs to improve system trans...   \n",
              "37  1.460000e+18  @BillyM2k How about my lil doge Floki? He woul...   \n",
              "1   1.280000e+18            @izzynobre Excuse me, I only sell Doge!   \n",
              "62  1.530000e+18  Tesla merch can be bought with Doge, soon Spac...   \n",
              "8   1.410000e+18          Release the Doge! https://t.co/9bXCWQLIhu   \n",
              "\n",
              "                                       cleaned_tweeet  year  month  day  \\\n",
              "26  working with doge devs to improve system trans...  2021      5   13   \n",
              "37  how about my lil doge floki? he would make a g...  2021     11    2   \n",
              "1                        excuse me, i only sell doge!  2020      7   18   \n",
              "62  tesla merch can be bought with doge, soon spac...  2022      5   27   \n",
              "8                                   release the doge!  2021      7    1   \n",
              "\n",
              "        time  labels                                 preprocessed_tweet  \n",
              "26  22:45:16       2  work doge devs improve system transaction effi...  \n",
              "37  20:36:20       1     lil doge floki would make great ceo judgment 🤌  \n",
              "1   00:53:43       0                                   excuse sell doge  \n",
              "62  15:27:21       2             tesla merch buy doge soon spacex merch  \n",
              "8   08:43:41       2                                       release doge  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3dce335-8335-4f3c-8d7a-1e9dd31505b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>cleaned_tweeet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>labels</th>\n",
              "      <th>preprocessed_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.390000e+18</td>\n",
              "      <td>Working with Doge devs to improve system trans...</td>\n",
              "      <td>working with doge devs to improve system trans...</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>22:45:16</td>\n",
              "      <td>2</td>\n",
              "      <td>work doge devs improve system transaction effi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.460000e+18</td>\n",
              "      <td>@BillyM2k How about my lil doge Floki? He woul...</td>\n",
              "      <td>how about my lil doge floki? he would make a g...</td>\n",
              "      <td>2021</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>20:36:20</td>\n",
              "      <td>1</td>\n",
              "      <td>lil doge floki would make great ceo judgment 🤌</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.280000e+18</td>\n",
              "      <td>@izzynobre Excuse me, I only sell Doge!</td>\n",
              "      <td>excuse me, i only sell doge!</td>\n",
              "      <td>2020</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>00:53:43</td>\n",
              "      <td>0</td>\n",
              "      <td>excuse sell doge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1.530000e+18</td>\n",
              "      <td>Tesla merch can be bought with Doge, soon Spac...</td>\n",
              "      <td>tesla merch can be bought with doge, soon spac...</td>\n",
              "      <td>2022</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>15:27:21</td>\n",
              "      <td>2</td>\n",
              "      <td>tesla merch buy doge soon spacex merch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.410000e+18</td>\n",
              "      <td>Release the Doge! https://t.co/9bXCWQLIhu</td>\n",
              "      <td>release the doge!</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>08:43:41</td>\n",
              "      <td>2</td>\n",
              "      <td>release doge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3dce335-8335-4f3c-8d7a-1e9dd31505b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3dce335-8335-4f3c-8d7a-1e9dd31505b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3dce335-8335-4f3c-8d7a-1e9dd31505b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos_df = pos_df.iloc[pos_train_n:pos_train_n+pos_test_n, 0:]\n",
        "test_neg_df = neg_df.iloc[neg_train_n:neg_train_n+neg_test_n, 0:]\n",
        "test_neu_df = neu_df.iloc[neu_train_n:neu_train_n+neu_test_n, 0:]\n",
        "test_df = test_pos_df.append([test_neg_df, test_neu_df]).sample(frac=1)\n",
        "test_df.labels = test_df.labels.replace({-1:0, 0:1, 1:2})\n",
        "\n",
        "print(len(test_df))\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68yrjDt4mQQn",
        "outputId": "bfca5c32-0ca1-48ed-8069-cecfebae4859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        tweet_id                                     original_tweet  \\\n",
              "22  1.400000e+18  How much is that Doge in the window? https://t...   \n",
              "25  1.390000e+18  @itsALLrisky Ideally, Doge speeds up block tim...   \n",
              "29  1.390000e+18                          The Dogefather\\nSNL May 8   \n",
              "40  1.370000e+18                                  @beeple 420M Doge   \n",
              "57  1.360000e+18                       No highs, no lows, only Doge   \n",
              "\n",
              "                                       cleaned_tweeet  year  month  day  \\\n",
              "22               how much is that doge in the window?  2021      5   20   \n",
              "25  ideally, doge speeds up block time 10x, increa...  2021      5   16   \n",
              "29                           the dogefather snl may 8  2021      4   28   \n",
              "40                                          420m doge  2021      3   15   \n",
              "57                       no highs, no lows, only doge  2021      2    4   \n",
              "\n",
              "        time  labels                                 preprocessed_tweet  \n",
              "22  10:41:00       2                                   much doge window  \n",
              "25  01:20:45       1  ideally doge speed block time x increase block...  \n",
              "29  06:20:47       1                                 dogefather snl may  \n",
              "40  23:11:50       2                                               doge  \n",
              "57  08:27:10       2                                     highs low doge  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2418efab-5544-41ef-b549-e2bda391ea37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>cleaned_tweeet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>labels</th>\n",
              "      <th>preprocessed_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.400000e+18</td>\n",
              "      <td>How much is that Doge in the window? https://t...</td>\n",
              "      <td>how much is that doge in the window?</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>10:41:00</td>\n",
              "      <td>2</td>\n",
              "      <td>much doge window</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.390000e+18</td>\n",
              "      <td>@itsALLrisky Ideally, Doge speeds up block tim...</td>\n",
              "      <td>ideally, doge speeds up block time 10x, increa...</td>\n",
              "      <td>2021</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>01:20:45</td>\n",
              "      <td>1</td>\n",
              "      <td>ideally doge speed block time x increase block...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.390000e+18</td>\n",
              "      <td>The Dogefather\\nSNL May 8</td>\n",
              "      <td>the dogefather snl may 8</td>\n",
              "      <td>2021</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>06:20:47</td>\n",
              "      <td>1</td>\n",
              "      <td>dogefather snl may</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1.370000e+18</td>\n",
              "      <td>@beeple 420M Doge</td>\n",
              "      <td>420m doge</td>\n",
              "      <td>2021</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>23:11:50</td>\n",
              "      <td>2</td>\n",
              "      <td>doge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1.360000e+18</td>\n",
              "      <td>No highs, no lows, only Doge</td>\n",
              "      <td>no highs, no lows, only doge</td>\n",
              "      <td>2021</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>08:27:10</td>\n",
              "      <td>2</td>\n",
              "      <td>highs low doge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2418efab-5544-41ef-b549-e2bda391ea37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2418efab-5544-41ef-b549-e2bda391ea37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2418efab-5544-41ef-b549-e2bda391ea37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_pos_df = pos_df.iloc[pos_train_n+pos_test_n:(pos_train_n+pos_test_n+pos_valid_n), 0:]\n",
        "valid_neg_df = neg_df.iloc[neg_train_n+neg_test_n:(neg_train_n+neg_test_n+neg_valid_n), 0:]\n",
        "valid_neu_df = neu_df.iloc[neu_train_n+neu_test_n:(neu_train_n+neu_test_n+neu_valid_n), 0:]\n",
        "valid_df = valid_pos_df.append([valid_neg_df, valid_neu_df]).sample(frac=1)\n",
        "valid_df.labels = valid_df.labels.replace({-1:0, 0:1, 1:2})\n",
        "\n",
        "print(len(valid_df))\n",
        "valid_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0bSmilkmDTO",
        "outputId": "ca3dd968-bc17-487a-e5a0-12f566b0b9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        tweet_id                                     original_tweet  \\\n",
              "3   1.420000e+18  @AshleyIllusion1 @lexfridman Lil X is hodling ...   \n",
              "7   1.410000e+18  Baby Doge, doo, doo, doo, doo, doo,\\nBaby Doge...   \n",
              "33  1.470000e+18  Tesla will make some merch buyable with Doge &...   \n",
              "31  1.380000e+18  @WSBChairman Doge Barking at the Moon – Miro h...   \n",
              "54  1.360000e+18  @itsALLrisky Doge appears to be inflationary, ...   \n",
              "\n",
              "                                       cleaned_tweeet  year  month  day  \\\n",
              "3   lil x is hodling his doge like a champ. litera...  2021      7   17   \n",
              "7   baby doge, doo, doo, doo, doo, doo, baby doge,...  2021      7    1   \n",
              "33  tesla will make some merch buyable with doge &...  2021     12   14   \n",
              "31                    doge barking at the moon – miro  2021      4   15   \n",
              "54  doge appears to be inflationary, but is not me...  2021      2    8   \n",
              "\n",
              "        time  labels                                 preprocessed_tweet  \n",
              "3   16:53:53       1  lil x hodling doge like champ literally never ...  \n",
              "7   09:24:21       2  baby doge doo doo doo doo doo baby doge doo do...  \n",
              "33  10:34:23       2           tesla make merch buyable doge amp see go  \n",
              "31  04:28:47       2                              doge bark moon – miro  \n",
              "54  05:27:36       2  doge appear inflationary meaningfully fix coin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-597197c0-0b12-424d-bb41-75c08738b319\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>original_tweet</th>\n",
              "      <th>cleaned_tweeet</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>time</th>\n",
              "      <th>labels</th>\n",
              "      <th>preprocessed_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.420000e+18</td>\n",
              "      <td>@AshleyIllusion1 @lexfridman Lil X is hodling ...</td>\n",
              "      <td>lil x is hodling his doge like a champ. litera...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>17</td>\n",
              "      <td>16:53:53</td>\n",
              "      <td>1</td>\n",
              "      <td>lil x hodling doge like champ literally never ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.410000e+18</td>\n",
              "      <td>Baby Doge, doo, doo, doo, doo, doo,\\nBaby Doge...</td>\n",
              "      <td>baby doge, doo, doo, doo, doo, doo, baby doge,...</td>\n",
              "      <td>2021</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>09:24:21</td>\n",
              "      <td>2</td>\n",
              "      <td>baby doge doo doo doo doo doo baby doge doo do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.470000e+18</td>\n",
              "      <td>Tesla will make some merch buyable with Doge &amp;...</td>\n",
              "      <td>tesla will make some merch buyable with doge &amp;...</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>10:34:23</td>\n",
              "      <td>2</td>\n",
              "      <td>tesla make merch buyable doge amp see go</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.380000e+18</td>\n",
              "      <td>@WSBChairman Doge Barking at the Moon – Miro h...</td>\n",
              "      <td>doge barking at the moon – miro</td>\n",
              "      <td>2021</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>04:28:47</td>\n",
              "      <td>2</td>\n",
              "      <td>doge bark moon – miro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1.360000e+18</td>\n",
              "      <td>@itsALLrisky Doge appears to be inflationary, ...</td>\n",
              "      <td>doge appears to be inflationary, but is not me...</td>\n",
              "      <td>2021</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>05:27:36</td>\n",
              "      <td>2</td>\n",
              "      <td>doge appear inflationary meaningfully fix coin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-597197c0-0b12-424d-bb41-75c08738b319')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-597197c0-0b12-424d-bb41-75c08738b319 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-597197c0-0b12-424d-bb41-75c08738b319');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "J06DvKXHObSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(utterance, tokenizer):\n",
        "    encoded = tokenizer(utterance, padding=\"max_length\", max_length=50, truncation=True)\n",
        "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
        "        encoded[key]=torch.LongTensor((encoded[key]))\n",
        "    return encoded[\"input_ids\"], encoded[\"attention_mask\"], encoded[\"token_type_ids\"]"
      ],
      "metadata": {
        "id": "c21plvHyTISN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataContext():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer=None\n",
        "        self.train_dataset=None\n",
        "        self.valid_dataset=None\n",
        "        self.test_dataset=None\n",
        "        self.train_dataloader=None\n",
        "        self.valid_dataloader=None\n",
        "        self.test_dataloader=None\n",
        "        self.df=None"
      ],
      "metadata": {
        "id": "8Trt8ev6WNrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PandasDataset(Dataset):\n",
        "\n",
        "    def __init__(self, context, dataset):\n",
        "\n",
        "        self.max_seq_length=50\n",
        "        self.tokenizer=context.tokenizer\n",
        "        self.df=context.df\n",
        "        self.context=context\n",
        "        self.lock=threading.Lock()\n",
        "        self.df=dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            self.lock.acquire()\n",
        "            row=self.df.iloc[idx]\n",
        "        finally:\n",
        "            self.lock.release()\n",
        "\n",
        "        x = tokenize(row[\"preprocessed_tweet\"], self.tokenizer)\n",
        "        if len(x)==3:\n",
        "            input_ids, attention_mask, token_type_ids=x\n",
        "        else:\n",
        "            raise Exception()\n",
        "\n",
        "        sample={\n",
        "            \"idx\": idx,\n",
        "            \"df_id\": row[\"tweet_id\"],\n",
        "            \"text\": row[\"preprocessed_tweet\"],\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "        }\n",
        "\n",
        "        sample[\"labels\"]=row[\"labels\"]\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            sample[\"token_type_ids\"]=token_type_ids\n",
        "\n",
        "        # delete data that we do not need because they confuse huggingfaces trainer\n",
        "        for key in list(sample.keys()):\n",
        "            if key not in ['input_ids', 'attention_mask', 'labels', 'token_type_ids']:\n",
        "                del sample[key]\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "KG5yza5KWRNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataContext(train_df, test_df, valid_df, batch_size,  shuffle_train=True) -> DataContext:\n",
        "  context = DataContext()\n",
        "  context.tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\", model_max_length=50)\n",
        "\n",
        "  context.train_dataset=PandasDataset(context, train_df)\n",
        "  context.test_dataset=PandasDataset(context, test_df)\n",
        "  context.valid_dataset=PandasDataset(context, valid_df)\n",
        "\n",
        "  num_workers=0\n",
        "  context.train_dataloader=DataLoader(context.train_dataset, batch_size=batch_size, shuffle=shuffle_train, num_workers=num_workers)\n",
        "  context.valid_dataloader=DataLoader(context.valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "  context.test_dataloader=DataLoader(context.test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return context"
      ],
      "metadata": {
        "id": "IlN2rRBuTdbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Trainer "
      ],
      "metadata": {
        "id": "iZW6YZupEPGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# export LOCAL_RANK=-1 "
      ],
      "metadata": {
        "id": "i2JZHUpRHWTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_base_model(num_labels, sub_dir, model_name, pretrained_model_path=None):\n",
        "  if pretrained_model_path is not None:\n",
        "    model=BertForSequenceClassification.from_pretrained(os.path.join(pretrained_model_path, sub_dir))\n",
        "  elif model_name == \"bert-base-uncased\":\n",
        "     config = BertConfig.from_pretrained(model_name,num_labels=num_labels)\n",
        "     model = BertForSequenceClassification(config)\n",
        "  elif model_name == \"distilbert-base-uncased\":\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "  else:\n",
        "    raise Exception(f\"unknown model \\\"{model_name}\\\"\")\n",
        "  \n",
        "  num_p=sum(p.numel() for p in model.parameters())\n",
        "  logging.info(f\"loaded model with {num_p:,} parameters.\")\n",
        "\n",
        "  return  model"
      ],
      "metadata": {
        "id": "P-4NTdfhKM4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a trainer, either with adapter or without\n",
        "def load_trainer(model, batch_size, num_epochs, learning_rate, output_dir, data_context):\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        logging_steps=200,\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        remove_unused_columns=False,\n",
        "        load_best_model_at_end=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_total_limit=None,\n",
        "        local_rank=-1,\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    def compute_accuracy(p: EvalPrediction):\n",
        "      # print(dir(p))\n",
        "      preds = np.argmax(p.predictions, axis=1)\n",
        "      return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "    trainer_args={\n",
        "        \"model\": model,\n",
        "        \"args\": training_args,\n",
        "        \"train_dataset\": data_context.train_dataset,\n",
        "        \"eval_dataset\": data_context.valid_dataset,\n",
        "        \"compute_metrics\": compute_accuracy,\n",
        "        \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.002)]\n",
        "    }\n",
        "\n",
        "\n",
        "    trainer=Trainer(**trainer_args)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "Jxu4luLkXgyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainning"
      ],
      "metadata": {
        "id": "0JoWW00hL_kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_context, batch_size, num_epochs, learning_rate, model_name, output_dir=\"/content/\"):\n",
        "  labels = [0, 1, 2]\n",
        "  model=load_base_model(len(labels), output_dir, model_name)\n",
        "  trainer=load_trainer(model, batch_size, num_epochs, learning_rate, output_dir, data_context)\n",
        "\n",
        "  starttime=time.time()\n",
        "  trainer.train()\n",
        "  duration=(time.time()-starttime)\n",
        "\n",
        "  logging.info(f\"training finished in {(duration/60):.2f} minutes\")\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  return model, trainer, duration"
      ],
      "metadata": {
        "id": "oPPPqaEaFjkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_experiment(learning_rate, batch_size, num_epochs, model_name, test_on_valid = False):\n",
        "  experiment_start_time=time.time()\n",
        "\n",
        "  def prediction_helper(results, y_ture):\n",
        "    y_pred=[p.argmax() for p in results.predictions]\n",
        "\n",
        "    acc=accuracy_score(y_true, y_pred)\n",
        "    f1=f1_score(y_true, y_pred, average=\"micro\")\n",
        "    return y_true, y_pred, acc, f1\n",
        "\n",
        "  logging.info(f\"train classifier!!\")\n",
        "  data_context = load_dataContext(train_df, test_df, valid_df, batch_size)\n",
        "  model, trainer, duration=train(data_context, batch_size, num_epochs, learning_rate, model_name)\n",
        "\n",
        "  model = model.to(model.device)\n",
        "  test_set=data_context.valid_dataset if test_on_valid else data_context.test_dataset\n",
        "  y_true=[i for i in test_set.df.labels]\n",
        "  starttime=time.time()\n",
        "  result=trainer.predict(test_set)\n",
        "  duration=(time.time()-starttime)\n",
        "  y_true, y_pred, acc, f1=prediction_helper(result, y_true)\n",
        "  print(f\"Model: {model_name}, Accuracy: {acc}, F1-score: {f1}, lr: {learning_rate}, batch_size: {batch_size}\")"
      ],
      "metadata": {
        "id": "XnzPasi9NaOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in [\"bert-base-uncased\", \"distilbert-base-uncased\"]:\n",
        "  for batch_size in [8,16,32,64,128,256,512]:\n",
        "      for learning_rate in [5e-3, 1e-4, 3e-4, 5e-4]:\n",
        "        single_experiment(learning_rate, batch_size, 100, model_name)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwUW7x0UO5kR",
        "outputId": "39b06cc8-1919-4e8d-fafa-da6e3b8cf414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/600 00:11 < 11:41, 0.84 it/s, Epoch 2/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.500362</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.599978</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-6 (score: 1.5003618001937866).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.005, batch_size: 8\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 18/600 00:15 < 09:26, 1.03 it/s, Epoch 3/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.963376</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910203</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.920830</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-18\n",
            "Configuration saved in /content/checkpoint-18/config.json\n",
            "Model weights saved in /content/checkpoint-18/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-12 (score: 0.910203218460083).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0001, batch_size: 8\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/600 00:10 < 09:58, 0.98 it/s, Epoch 2/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.248428</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.552096</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-6 (score: 1.2484277486801147).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0003, batch_size: 8\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 24/600 00:20 < 08:53, 1.08 it/s, Epoch 4/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.077692</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.015687</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.918898</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.942688</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-18\n",
            "Configuration saved in /content/checkpoint-18/config.json\n",
            "Model weights saved in /content/checkpoint-18/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/checkpoint-24\n",
            "Configuration saved in /content/checkpoint-24/config.json\n",
            "Model weights saved in /content/checkpoint-24/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-18 (score: 0.9188984632492065).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0005, batch_size: 8\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/300 00:20 < 09:48, 0.49 it/s, Epoch 4/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.278138</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.403498</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.345056</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.505598</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-3\n",
            "Configuration saved in /content/checkpoint-3/config.json\n",
            "Model weights saved in /content/checkpoint-3/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-9\n",
            "Configuration saved in /content/checkpoint-9/config.json\n",
            "Model weights saved in /content/checkpoint-9/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-9 (score: 1.345056414604187).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.005, batch_size: 16\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 15/300 00:25 < 09:29, 0.50 it/s, Epoch 5/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.192184</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.935741</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.919915</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.906889</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.930316</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-3\n",
            "Configuration saved in /content/checkpoint-3/config.json\n",
            "Model weights saved in /content/checkpoint-3/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-9\n",
            "Configuration saved in /content/checkpoint-9/config.json\n",
            "Model weights saved in /content/checkpoint-9/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-15\n",
            "Configuration saved in /content/checkpoint-15/config.json\n",
            "Model weights saved in /content/checkpoint-15/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-12 (score: 0.9068887233734131).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0001, batch_size: 16\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/300 00:20 < 09:57, 0.48 it/s, Epoch 4/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.024374</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.912875</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.902286</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.080253</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-3\n",
            "Configuration saved in /content/checkpoint-3/config.json\n",
            "Model weights saved in /content/checkpoint-3/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-9\n",
            "Configuration saved in /content/checkpoint-9/config.json\n",
            "Model weights saved in /content/checkpoint-9/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-9 (score: 0.9022858142852783).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0003, batch_size: 16\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 15/300 00:25 < 09:26, 0.50 it/s, Epoch 5/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.966084</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.391456</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.929950</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.919690</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.214597</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-3\n",
            "Configuration saved in /content/checkpoint-3/config.json\n",
            "Model weights saved in /content/checkpoint-3/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-9\n",
            "Configuration saved in /content/checkpoint-9/config.json\n",
            "Model weights saved in /content/checkpoint-9/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to /content/checkpoint-15\n",
            "Configuration saved in /content/checkpoint-15/config.json\n",
            "Model weights saved in /content/checkpoint-15/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-12 (score: 0.9196901321411133).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0005, batch_size: 16\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/200 00:09 < 16:10, 0.20 it/s, Epoch 2/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>5.208806</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.511546</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-2\n",
            "Configuration saved in /content/checkpoint-2/config.json\n",
            "Model weights saved in /content/checkpoint-2/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-4\n",
            "Configuration saved in /content/checkpoint-4/config.json\n",
            "Model weights saved in /content/checkpoint-4/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-2 (score: 5.208806037902832).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.005, batch_size: 32\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/200 00:31 < 09:51, 0.32 it/s, Epoch 6/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.249812</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.136329</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.992666</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.918667</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.904516</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.910376</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-2\n",
            "Configuration saved in /content/checkpoint-2/config.json\n",
            "Model weights saved in /content/checkpoint-2/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-4\n",
            "Configuration saved in /content/checkpoint-4/config.json\n",
            "Model weights saved in /content/checkpoint-4/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-8\n",
            "Configuration saved in /content/checkpoint-8/config.json\n",
            "Model weights saved in /content/checkpoint-8/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-10\n",
            "Configuration saved in /content/checkpoint-10/config.json\n",
            "Model weights saved in /content/checkpoint-10/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-10 (score: 0.9045159816741943).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0001, batch_size: 32\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  8/200 00:30 < 16:06, 0.20 it/s, Epoch 4/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.050743</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.486931</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.004171</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.031181</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-2\n",
            "Configuration saved in /content/checkpoint-2/config.json\n",
            "Model weights saved in /content/checkpoint-2/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-4\n",
            "Configuration saved in /content/checkpoint-4/config.json\n",
            "Model weights saved in /content/checkpoint-4/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-8\n",
            "Configuration saved in /content/checkpoint-8/config.json\n",
            "Model weights saved in /content/checkpoint-8/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-6 (score: 1.004171371459961).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.5714285714285714, F1-score: 0.5714285714285714, lr: 0.0003, batch_size: 32\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12/200 01:26 < 27:11, 0.12 it/s, Epoch 6/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.988637</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.000713</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.604743</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.122078</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.076573</td>\n",
              "      <td>0.384615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.120869</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-2\n",
            "Configuration saved in /content/checkpoint-2/config.json\n",
            "Model weights saved in /content/checkpoint-2/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-4\n",
            "Configuration saved in /content/checkpoint-4/config.json\n",
            "Model weights saved in /content/checkpoint-4/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-6\n",
            "Configuration saved in /content/checkpoint-6/config.json\n",
            "Model weights saved in /content/checkpoint-6/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-8\n",
            "Configuration saved in /content/checkpoint-8/config.json\n",
            "Model weights saved in /content/checkpoint-8/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-10\n",
            "Configuration saved in /content/checkpoint-10/config.json\n",
            "Model weights saved in /content/checkpoint-10/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to /content/checkpoint-12\n",
            "Configuration saved in /content/checkpoint-12/config.json\n",
            "Model weights saved in /content/checkpoint-12/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-10 (score: 1.0765725374221802).\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "***** Running Prediction *****\n",
            "  Num examples = 14\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: bert-base-uncased, Accuracy: 0.35714285714285715, F1-score: 0.35714285714285715, lr: 0.0005, batch_size: 32\n",
            "\n",
            "bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 41\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  3/100 00:05 < 08:36, 0.19 it/s, Epoch 2/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.750160</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.659176</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/checkpoint-1\n",
            "Configuration saved in /content/checkpoint-1/config.json\n",
            "Model weights saved in /content/checkpoint-1/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/checkpoint-2\n",
            "Configuration saved in /content/checkpoint-2/config.json\n",
            "Model weights saved in /content/checkpoint-2/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/checkpoint-1 (score: 2.7501604557037354).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4063945a6b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5e-4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msingle_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-67561f787c6b>\u001b[0m in \u001b[0;36msingle_experiment\u001b[0;34m(learning_rate, batch_size, num_epochs, model_name, test_on_valid)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train classifier!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdata_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-2fd2b6d5b1c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_context, batch_size, num_epochs, learning_rate, model_name, output_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mstarttime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstarttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrainOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             )\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# A Callback can skip the return of `control` if it doesn't change it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         self.training_tracker.update(\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {int(state.epoch)}/{state.num_train_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, value, force_update, comment)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_remaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate_bar\u001b[0;34m(self, value, comment)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;34mf\" {format_time(self.predicted_remaining)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             )\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\", {1/self.average_time_per_item:.2f} it/s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"]\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\", {self.comment}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r8GXLP0qbPRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}